Using STACKS v. 2.41 from https://catchenlab.life.illinois.edu/stacks/ (Catchen et al. 2013)
J. Catchen, P. Hohenlohe, S. Bassham, A. Amores, and W. Cresko. Stacks: an analysis tool set for population genomics. Molecular Ecology. 2013. [reprint]

Notes from my runs (things that went wrong, what worked to fix issues, and what I used):

Coding Pipeline Log - Also see the READme file from Sam Greaves for all commands.

9/25/21 – Received Seq. data back from Illumina BaseSpace
Want files in .fastq format, not SAV
Need to download the BaseSpace Seq. Hub Downloader
Did this using BaseSpace downloader (regular one from BaseSpace website) 
*important* - must get access to the PROJECT, not just the ‘run’ to be able to download the whole PROJECT (needed), and not just the run. Was able to select the .fastq.gz file type and my personal external hard drive to download the files (~168 GB of data)

Next, I need to copy over these files from my external hard drive (D:\) to command line server on coombs:

So to copy files over, you need to use:
scp mgilg@coombs.cs.ucf.edu:/home/mgilg/fundulus/info/’filename’ ./
This puts the files in the C drive (C:\\Users\drew)
This command needs to be run in the Windows command prompt while you are connected to the VPN but NOT PuTTY. I got this to work without error for copying 4 out of 6 files that were in this folder. For the last two, it went back to not recognizing the host name mgilg@coombs.cs.ucf.edu... Not sure what to do here. But you can open these .tsv files with excel, except they didn’t have any useful information in them…??
Fixed this: you have to make sure Putty can BE opened (get to the point where you can load coombs from Putty even though you don’t log in) – if putty is saying it cant connect to host when you try to log on, command prompt will not be able to connect either. Also, I pulled the other 2 files and looked through, there are more helpful files in the /optimize folder in /fundulus. Also pull the sample_read_stats.tsv file out of the /fundulus folder – this one is the one I think.
The only file it couldn’t find was the SNP distribution plot in the /optimize folder. It could access coombs but just couldn’t find the file name, maybe a wrong file name. Try again later.  

Example on how to copy over 1 of these .gz files from my hard drive to the server:
In Windows Command Prompt:
-	Cd to your external hard drive by typing ‘D:’ and hitting enter
-	Cd into your folder containing the data dump from the downloader (the output folder you selected in the downloader)
-	Open up file explorer and find the exact file path to the first .gz file (I think it was 3 or 4 folders in).
-	Right click to copy the file address in the box up top, then paste this into a .txt document on notepad. You’ll need to change the backslashes to regular slashes in command prompt for command line to recognize the folders.
-	Cd into this folder in command prompt (the folder just before the .gz files)
-	In file explorer, click to change the name of your .gz file and then just copy over the name of this .gz file in its entirety. Put this at the end of your file path in the .txt doc. Copy over the rest of this from .txt to command prompt. 
-	Run this command in Windows command prompt: “scp FILE_NAME mgilg@coombs.cs.ucf.edu:/home/mgilg/file_path_you_want
-	For file_path_you_want, I used /home/mgilg/round2/rawseqdata
-	It’ll have you enter the password for Matt’s virtual server, then it starts to copy over this file
-	To copy files the reverse way, just switch the file path and destinations. For example, to copy over the sample_read_stats.tsv file, you’d run ‘scp mgilg@coombs.cs.ucf.edu:/home/mgilg/round3/sample_read_stats.tsv D:’ to put the file in the external hard drive. 
To run this to copy over all .gz files at once from the \newseqdata folder on the hard drive, we just copied over the entire base folder (everything in them – which should be a bunch of empty folders and then the .gz files) using this command in command prompt:
‘scp -r ./NS2438-MGilg_S2_HNY7TDMXX_Lane1-2-293028970/ mgilg@coombs.cs.ucf.edu:/home/mgilg/round2/rawseqdata’

Then entering the password. This will copy each file over sequentially, and it is going to take a longggggg long time. Computer must be still connected to the internet and the power to work. 
I lost connection due to being idle on coombs, so it kicked me off after like 30 minutes of inactivity. So I had to go through and manually copy over each subfolder with the .gz files in it with a similar command to the one used above. 

Useful command:
‘du -bsh *’ provides the file sizes of all files in the current directory

After I finished copying over all of the files from my external hard drive to coombs,
From each subdirectory (containing the .gz files):
Used the command ‘mv *.gz ../../../’ to move all of the .fastq.gz files in subfolders within subfolders to one main folder (rawseqdata). Now all files are on both my hard drive and in the coombs server under ~/home/mgilg/round2/rawseqdata. 

Before you start the pipeline, you need to copy over multiple directories from Sam or from our old stuff. These include the 'info', 'scripts', and make a new, empty 'raw' and 'logs' folder. Sometimes if you don't have these some programs will throw errors.

1.	Started running STACKS on new data – ran clonefilter first to remove PCR dups.

2.	For the demux.sh step (next step), you have to go in and edit the SetB_barcodes.tsv file to update the new barcodes and sample names you are using. Basically, demux.sh uses this file to search seqs for your adapter and barcodes and then re-labels your sequences as your sample names. The format for this file is  a tab-spaced value format: 1st column is the 6 bp seq, then a tab, then 2nd column is the 8 bp seq, then a tab, then the sample name you want to use. Remember, these sample names can NOT have spaces in them. Use _ instead. I renamed my new .tsv file so we didn’t overwrite the original. 
To edit files, you usually want to do this in command line using the 'vim' command, since this does not change the basic file type and and organization. 
Once, I took a catalog file (.tsv) from the server and copied it over to my hard drive, opened it up with excel, changed a few things, then saved it back as a .tsv and re-uploaded it to the server. Something about the file had corrupted on the re-upload and I wasnt able to use it.
How to use vim:
1. Be careful while using vim because I've found its pretty easy to delete your whole file, or corrupt it so it can no longer be used. 
2. First, I'd use 'cat filename' to see what file I'm looking at and everything thats inside of it, to make sure its the file I want to edit.
3. Then, I'd use 'vim filename' to enter the editing portion of the file. This looks and acts very similar to 'cat' as it shows everything in the file. At this point, you can't delete or change anything.
4. If you type 'i' while in vim mode, you enter 'insert' mode. Now any changes you make can be permenant, so be careful here. 
5. Delete and make changes you need to make (can paste in newly created catalogs separated by tabs from .txt documents), and then hit escape to leave the insert mode. 
6. To get out of 'vim' you need to 'write' your changes to the file. You can do this by typing ':w' and pressing enter.
7. To write and quit (leave vim), use ':wq', then enter.
The file should now be changed how you wanted it. If you don't write the changes and quit properly, or if you don't leave insert mode, command line freaks out and you might lose the file.

Other note: after running this command successfully, make sure to write down your job ID # on a .txt file. Check it using ‘sacct -u mgilg’ or “sacct -j <job number>” which checks status of all jobs submitted by mgilg in past 24 hours. Or you could use ‘squeue’ to check current jobs in the server. 
Also a quick note: if something that usually takes multiple hours to run only took a few minutes, something almost definitely went wrong. Check your 'logs' directory and use 'cat filename' to read the log files and figure out what went wrong. 

3.	For the gawk command (next command) on 3/30, make sure to change the job ID # from 229630 at the very end to your job ID from step #2.
4.	Trim and quality filter step (3/31): Again, make sure to change barcodes file to your current/updated one. 

(here I messed up a 'vim' and accidentally deleted a hidden file) - Troubleshooting: issues with editing .bashrc using vim and issues copying over a new .bashrc file cuz you deleted the original. 
Stuff we tried: ls -a to see hidden files – the bash.rc file specifically. This looks like you deleted the info in this file, so its now empty. 

Sam informed me that there is actually a backup copy at this address: /etc/skel/.bashrc
So, from the home folder (~), run this: cp /etc/skel/.bashrc ./

And you should be able to copy this file over. 

Then need to go back in using vim .bashrc and add the stuff to the end of the file in Sam’s email. 
Pretty sure the error (exceeding Disk Quota/no space) is caused because I was at ~980 Gbs and the hard quota limit is 1 TB. I deleted the practice folder from mgilg which cleared up about 230 GBs, now were at 745. Still throwing the same error though. 
Another note: if you ever get a disk quota exceeded error - you need to talk to whoever runs the server and have them up your data limit. They have a 'soft' and a 'hard' limit - the soft limit comes into play 1 week after you've passed the soft limit. The hard limit cannot be passed  

10/19/21 – after upping my disk space quota via Steve, I re-copied over the .bashrc backup file to /home/mgilg successfully, then successfully vim’ed into it and edited it using “i” then inserting the code Sam sent (export PATH=”/home/greaves/.bin:$PATH”) then hitting escape to exit insert mode, then typing :wq (write, quit), which actually WORKED this time!!! Double checked the file wrote successfully using vim .bashrc again

Sooo, now back to trim and quality filer steps – 3/31/21 step – ran, got a job ID, but when checked the queue there wasn’t anything there… this step took 14 minutes when Sam did it so something might be wrong here
Re-ran the 3/31 – 4/1 steps and did not get the same errors of the ‘fastp.slurm could not recognize command’. Instead, now I am getting this error:
Processing BL_154:
.1.fq.gz: No such file or directory
.2.fq.gz: No such file or directory
 .1.fq.gz: No such file or directory
.2.fq.gz: No such file or directory
-bash: 0 / 0: division by 0 (error token is “0”)
Probably an error in file paths somewhere and it cant find these files…
Emailed Sam for further help
‘Du -h’  - used to check disk quota and file space on my account

Another fix – to fix this error (showed up in the log files for fastp.slurm):
Call: /var/spool/slurm/slurmd/job290539/slurm_script -i raw/demux -b info/SetB_barcodes_3.tsv
FN_95
Filtering and Trimming
Error to read gzip file
Error to read gzip file
ERROR: file 'raw/demux/FN_95^M.1.fq.gz' doesn't exist, quit now
Discarding wrong length for read 1
Error to read gzip file
ERROR: file 'fastp_report/FN_95^M_intermediate/FN_95^M.1.fq.gz' doesn't exist, quit now
Discarding wrong length for read 2
Error to read gzip file
ERROR: file 'fastp_report/FN_95^M_intermediate/FN_95^M.2.fq.gz' doesn't exist, quit now
 
Ignore highlighted region, this is not important. 
To fix this, basically, you modified the barcodes file (.tsv) in windows (on excel or notepad) after downloading it from command line server. Because of this, when you saved the edits and re-uploaded the file into command line again, the file saved with the Windows version of line endings, not the linux version of line endings (this is where the ^M comes in). The slurm program gets confused with this and can’t read the file. So what you have to do to fix this is run this command in round2/info:
‘dos2unix SetB_barcodes_3.tsv’
Then it’ll say: “dos2unix: converting file SetB_barcodes_3.tsv to Unix format...”
Then I re-ran the trim and quality filter steps and it submitted a job to the queue that I can actually see in the queue – ID #290760 on the successful try. This step SHOULD take a little while, if your ID doesn’t show up in the queue after running, something is wrong, even if you check your status using sacct -u.
10/20/21: So it took about 45 minutes to complete in the queue, with subsections. This seems like it worked okay, so I moved onto the 4/1/21 command. Changed the barcodes file to the _3.tsv one in the command, then ran. It is progressing nicely with no errors so far.
Note for the popmap_catalog command in 4/1 (2nd command):
Created a changed popmap catalog using 'vim' - Needed representation from all populations, but only ~5 samples from each pop for STACKS to really call all the loci properly. So I just edited the current popmap_catalog.tsv and changed current samples to 5 of each pop from this run, and then had everything a '1'.
To edit in vim, hit 'i', then carefully delete what you want, then copy in what you need to replace, then hit :wq (write, quit) and it will save changes and leave the file. double check it worked using cat (file)
It looks like the ustacks.coverages script, run later on at the end of the 4/15/21 command, creates the mean coverage and max number of reads columns that you need to copy over to the sample_read_stats folder.  

10.25.21 - optimize finally finished after about 6 days of queue

To correctly choose optimization parameters, use "Lost in Parameter Space: a Road Map for STACKS" by Paris, Stevens, and Catchen 2017.

So I went in and changed Gilg_popmap to all current 287 samples being used this run - all assinged to same pop (1)

Then I determined M should be run at 3 instead of 4 this time around. So n is also run at 3, and m is run at 2. 
Changed these in the following command, left l = 8, then ran ustacks.sh.

Job ID# 292350

and its in the queue

Forgot to fix 1 issue with naming 1 sample – forgot 1 _, so corrected the file and re-ran this command 
Job ID # 292669
This time it worked no errors, so I ran ./scripts/ustacks_coverages.sh 292669 denovo_M3n3m2 and it output the file in this folder np. 
Next, I ran the 4/22 command, when it looks like I didn’t really need to. Ustacks had already stripped the .1 off of the file names, so what happened is I had a couple of TB samples (2) named as TB 2.1 and 3.1, and this command recognized those and stripped the .1’s off of them. They are now labeled as TB 2 and TB 3… just remember this for next time
Running the second ustacks command: I created a new popmap (popmap_filtered.tsv) in place of the popmap_catalog2.tsv that Sam created. This popmap_filtered file contains only those samples >10x coverage and > 1,000,000 reads (the READme is incorrect in saying only samples <1000000 reads or 10x coverage). Also, Sam said to only run the quality samples in the cstacks command, so I didn’t use the old popmap_catalog here. Need to talk to Gilg about lowering this threshold a little bit to include more samples, but for now I’m running this part. 

We ended up lowering our quality thresholds to more than 8x coverage and 1M reads from the ustacks 'sample_read_stats' file. This file shows us both mean coverage and total number of reads for each individual after optimization. 

One more random troubleshooting note: for any scripts or files in 'info' or 'scripts', if they're blue or green they are excecutable (able to be run). If they are white I wouldn't trust them to run properly. To make a file executable in command line, use 'chmod +x {filename}' which should turn it green.

From here on I moved on to our 3rd round of sequencing data, and then finally combined all seq. data and did one big run through the STACKS pipeline. Everything below is my notes on our combined dataset run through the stacks pipeline from the optimize step through the 'populations' step.
 
 M n m  r loci_total loci_kept variant_sites change_variant dd_variant
1 1 1 3 80    3284096     18723        162138         162138         NA
2 2 2 3 80    2718139     24916        257876          95738      66400
3 3 3 3 80    2457375     28130        313335          55459      40279
4 4 4 3 80    2314574     29518        342054          28719      26740
5 5 5 3 80    2227121     30073        354561          12507      16212
6 6 6 3 80    2165109     30192        358472           3911       8596
7 7 7 3 80    2121876     30177        358816            344       3567
8 8 8 3 80    2084373     30048        357413          -1403       1747
9 9 9 3 80    2058545     29969        356575           -838        565


Output for optimize ^

Updated the catalog and used popmap_catalog.tsv for the ustacks catalog (next step)
Job ID for ustacks: 328909

Finished, but about 2/3rds of the TB samples failed due to bad naming in their popmap
I updated the popmap with the correct names and hopefully this fixes this error - the ustacks command was unable to find the proper TB files in the ./cleaned folder, so it only did about 11 TB samples.
Re-ran with job ID: 329398

4/22/21 step
I accidentally stripped off the .1. in two samples files (6 files total, 3 per sample ID)
These were originally TB_2.1 and TB_3.1, and these are NOW NAMED TB_2 and TB_3.

3/16/2022
Ran cstacks command with job ID of 329988. Used the popmap_catalog.tsv file - but forgot to change catalog names of TB_2.1 and 3.1, so need to re-run cstacks.
Re-ran with updated popmap with job ID of 329994

3/18/2022
cstacks did not work properly... I think it was trying to use the old popmap (popmap_catalog2.tsv) instead of popmap_catalog.tsv
I made sure it referenced the proper catalog and ran it again with job ID: 330376

cstacks is running considerably longer than it has in the past... Thought there may be an issue but luckily it is creating a log file and updating it every once in a while (i think). 
I checked the log file and everything seems to be running fine, it's been almost 3 days straight and its on sample 253/460 or something like that. No errors so far as long as I can tell

cstacks took ~8 days to run... but finally finished and looks like it has no errors. Checked the log file for run 330376 and we have 'Writing catalog in directory './denovo_M4n4m3'. Final 
catalog contains 7,013,615 loci. cstacks is done. 

3/26/22
Started the proc_samples.slurm script command with job ID 331677

Proc samples had two errors that I could tell (see screenshots in word walkthrough), one was I needed to rename a file in the ./cleaned folder that was causing issues (I think) - this WAS TB_2.2.1.fq.gz and TB_2.2.2.gq.gz and I changed these to TB_2_2.1.fq.gz and .2.fq.gz respectively
I also changed these in the popmap and the './denovoM...' folder
Then I changed the name of the TB_3 and TB_2 (formerly TB_3.1 and TB_2.1 respectively) in the ./cleaned folder since they were still original names and I figured this would cause an issue since procsamples.slurm is using the ./cleaned folder too. 
Lastly, I had another issue: failed to open 'SH_22.matches.bam' file, ERROR, you might need to increase your systems open-max-file limit, and see this website for more information: https://groups.google.com/g/stacks-users/c/GZqJM_WkMhI/m/m9Hreg4oBgAJ
After going to that site, I saw we needed to up the max limit from 1024 to 4096 and it should work.
did this using 'ulimit -n' to check for 1024, then confirmed, then ran 'ulimit -Hn' to find the max limit i could set it to, which was 200,000, then I ran 'ulimit -n 4096' on /home/mgilg, then re-ran proc.samples.slurm with a Job ID #331685
Job ID # again is 331685 for 2nd proc_samples.slurm run.

proc_samples.slurm re-run is going well, progressed through both of those error messages and it has been running for ~7 hours now

Looks like proc_samples ran well, no issues that I could see, so I ran populations with these flags: -r 1 (for loci must be present in at least 100% of individuals in each population), the -O flag with output directory specified, the -M flag with popmap_catalog.tsv, the --vcf flag to generate a .vcf file, and the --write-single-snp flag for 1 snp per locus
Job ID for populations: 331696
Populations ran, but ignored some of my flags. It included the --write single snp, the --vcf, and got the right -M catalog, but went back to default -O and put the files in a directory ./denovoM4n4m3/output_r80, and it also ignored the -r flag of 1.0, defaulting to 0.8 (80%)...

